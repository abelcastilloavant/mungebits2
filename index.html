<!DOCTYPE html>
<html lang="en" class="">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="description" content="A way of thinking about data preparation that couples the definition of what happens in batch processing versus online prediction so that both can be described by the same codebase. With mungebits, you can save time on having to re-implement your R code to work in production and instead re-use the same codebase.">

    <title>mungebits2 (http://github.com/robertzk/mungebits2)</title>

    <link rel="stylesheet" media="all" href="stylesheets/rocco.css" />
    <link rel="stylesheet" media="all" href="stylesheets/github-markdown.css" />

    <script src="assets/highlight.pack.js"></script>
    <script type="text/javascript">
      hljs.initHighlightingOnLoad();
    </script>

    <style type="text/css">
      .header {
        position: fixed;
        top: 0px;
        width: 100%;
        background-color: rgba(0, 0, 0, 0.25);
        padding: 10px;
      }
      
      .header a {
        padding-right: 30px;
      }

      .container {
        margin-top: 40px;
      }

      body {
        padding: 0;
        margin: 0;
      }

      div.code-background {
        float: right;
        position: fixed;
        z-index: -1;
        height: 100%;
        background-color: #f8f8ff;
        width: 60%;
        right: 0px;
      }

      div.section {
        clear: both;
        margin: 0; padding: 0;
      }

      div.code {
        float: right;
        width: 60%;
      }

      code.R {
        font-size: 1.2em;
        line-height: 2em;
        margin-top: 0em;
        margin-bottom: -2em;
        padding-top: 0;
        margin-top: -1em;
      }

      code.R > span.spacer {
        position: relative;
      }

      div.code > pre {
        margin: 0;
        padding-left: 2em;
        margin-top: 0;
        margin-bottom: 0;
      }

      div.markdown {
        padding: 1em;
        padding-top: 0;
        background: #fff;
        float: left;
        width: 35%;
      }
    </style>

  </head>

  <body>
    <div class="header">
      <a href="https://github.com/robertzk/rocco">
        <img id="rocco-logo" src="https://img.shields.io/badge/Generated by rocco_v0.1.1-%E2%9C%93-blue.svg"/>
      </a>
    </div>
    <div class="container">

      <div class="code-background"></div>

        <div class="section">
          <div class="markdown markdown-body">
            <h1>column_transformation.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Column transform.
#' @param ... Random.
#' @export
column_transformation <- function(...) identity(...)</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>debug.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Debugging the train and predict function of a mungebit should be
transparent to the user. Say we have a mungebit called <code>value_replacer</code>.
By calling <code>debug(value_replacer)</code>, we should be able to simultaneously
set debug hooks on both the <code>train_function</code> and <code>predict_function</code>
of the mungebit. Calling <code>undebug(value_replacer)</code> will remove the hooks.</p>

<p>R has a tremendous array of debugging tools. You should familiarize
yourself with them to make your life much simpler. A great resource
is chapter 8 of <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">The R Inferno</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Generic debugging.
#'
#' @inheritParams base::debug
#' @seealso \code{\link[base]{debug}}
#' @export
debug <- function(fun, text = "", condition = NULL) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>The standard <a href="http://adv-r.had.co.nz/OO-essentials.html">S3 generic</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  UseMethod("debug")
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>By default, debugging should preserve the behavior from the base package.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' @export
debug.default <- function(fun, text = "", condition = NULL) {
  base::debug(fun, text = "", condition = NULL)
}

#' @export 
debug.mungebit <- function(fun, text = "", condition = NULL) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>To debug a mungebit, we loop over the train and predict functions
and set their internal debugging flag. The <code>if</code> statement is
necessary in case either are <code>NULL</code> (e.g., there is no train
or predict step).</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  for (fn in list(fun$.train_function, fun$.predict_function)) {
    if (is.function(fn)) {
      debug(fn, text, condition)
    }
  }
}

#' @export 
debug.mungepiece <- function(fun, text = "", condition = NULL) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>To debug a mungepiece, we delegate all the work to the mungebit.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  debug(fun$mungebit(), text, condition)
}

#' Generic removal of debugging.
#'
#' @inheritParams base::undebug
#' @seealso \code{\link[base]{undebug}}
#' @export
undebug <- function(fun) {
  UseMethod("undebug")
}

#' @export
undebug.default <- function(fun) {
  base::undebug(fun)
}

#' @export 
undebug.mungebit <- function(fun) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>To undebug a mungebit, we loop over the train and predict functions
and unset their internal debugging flag. The <code>if</code> statement is
necessary in case either are <code>NULL</code> (e.g., there is no train
or predict step), and to avoid throwing a warning if the function
isn&#39;t already being debugged.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  for (fn in list(fun$.train_function, fun$.predict_function)) {
    if (is.function(fn) && isdebugged(fn)) {
      undebug(fn)
    }
  }
}

#' @export 
undebug.mungepiece <- function(fun) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>To undebug a mungepiece, we delegate all the work to the mungebit.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  undebug(fun$mungebit())
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>munge.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' @export
munge <- NULL</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-initialize.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Constructor for mungebit class.
#'
#' Mungebits are atomic data transformations of a data.frame that,
#' loosely speaking, aim to modify "one thing" about a variable or
#' collection of variables. This is pretty loosely defined, but examples
#' include dropping variables, mapping values, discretization, etc.
#'
#' @param train_function function. This specifies the behavior to perform
#'    on the dataset when preparing for model training. A value of NULL
#'    specifies that there should be no training step, i.e., the data
#'    should remain untouched.
#' @param predict_function function. This specifies the behavior to perform
#'    on the dataset when preparing for model prediction. A value of NULL
#'    specifies that there should be no prediction step, i.e., the data
#'    should remain untouched.
#' @param enforce_train logical. Whether or not to flip the trained flag
#'    during runtime. Set this to FALSE if you are experimenting with
#'    or debugging the mungebit.
#' @examples
#' mb <- mungebit$new(column_transformation(function(column, scale = NULL) {
#'   # `trained` is a helper provided by mungebits indicating TRUE or FALSE
#'   # according as the mungebit has been run on a dataset.
#'   if (!trained) {
#'     cat("Column scaled by ", input$scale, "\n")
#'   } else {
#'     # `input` is a helper provided by mungebits. We remember the
#'     # the `scale` so we can re-use it during prediction.
#'     input$scale <- scale
#'   }
#'   column * input$scale
#' }))
#' 
#' # We make a lightweight wrapper to keep track of our data so
#' # the mungebit can perform side effects (i.e., modify the data without an
#' # explicit assignment <- operator).
#' irisp <- list2env(list(data = iris))
#' #mb$run(irisp, 'Sepal.Length', 2)
#'
#' #head(mp$data[[1]] / iris[[1]])
#' # > [1] 2 2 2 2 2 2
#' #mb$run(mp, 'Sepal.Length')
#' # > Column scaled by 2
#' #head(mp$data[[1]] / iris[[1]])
#' # > [1] 4 4 4 4 4 4 
mungebit_initialize <- function(train_function   = base::identity,
                                predict_function = train_function,
                                enforce_train    = TRUE) {
  stopifnot(isTRUE(enforce_train) || identical(enforce_train, FALSE))

  if (!is.acceptable_function(train_function)) {
    stop("To create a new mungebit, please pass a ",
         sQuote("function"), " as the first argument. I received ",
         "something of class ", sQuote(crayon::red(class(train_function)[1L])), ".")
  }

  if (!is.acceptable_function(predict_function)) {
    stop("To create a new mungebit, please pass a ",
         sQuote("function"), " as the second argument. I received ",
         "something of class ", sQuote(crayon::red(class(second_function)[1L])), ".")
  }

  self$.train_function   <- train_function
  self$.predict_function <- predict_function
  self$.input            <- new.env(parent = emptyenv())
  self$.trained          <- FALSE
  self$.enforce_train    <- enforce_train
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-run.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Imagine running an imputation script on a dataset. 
On a training set, we have to compute the mean and replace
the <code>NA</code>s with its value. However, when a single row comes
in through a streaming production system, we merely need to
memorize the computed mean and replace a variable with it
if it is <code>NA</code>.</p>

<p>Calling the <code>run</code> method on a mungebit will store any
data it needs for production, such as imputed means,
in the <code>input</code> member. The second time <code>$run</code> is called
(i.e., during prediction or real-time production use),
it will be using the <code>predict_function</code> rather than the
<code>train_function</code>, which will be less computationally expensive
since it does not have to operate in reference to a training set
and can use the memorized results in <code>input</code> to achieve
the same transformation as the <code>train_function</code>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Run a mungebit.
#' 
#' Imagine flipping a switch on a set of train tracks. A mungebit
#' behaves like this: once the \code{trained} switch is flipped,
#' it can only run the \code{predict_function}, otherwise it will
#' run the \code{train_function}.
#'
#' @rdname mungebit
#' @param data environment or data.frame. Essentially an environment
#'   containing a \code{data} variable. In this case, that \code{data} variable
#'   will have a side effect enacted on it. If a \code{data.frame}, then 
#'   the return value will be the modified \code{data.frame} and the mungebit
#'   will record any results it must memorize in its \code{input}.
#' @param ... additional arguments to the mungebit's \code{train_function} or
#'   \code{predict_function}.
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}.
mungebit_run <- function(data, ...) {
  if (is.environment(data)) {
    if (!exists("data", envir = data, inherits = FALSE)) {
      stop("If you are passing an environment to a mungebit, you must ",
           "provide one that contains a ", sQuote("data"), " key.")
    }

    data$data <- self$run(data$data, ...)
  } else if (isTRUE(self$.trained)) {
    data <- self$predict(data, ...)
  } else {
    data <- self$train(data, ...)  
  }
  data
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-train_predict.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Run the train function on a mungebit.
#'
#' The train function is responsible for performing a munging step and
#' storing metadata that can replicate the munging step in a live
#' production environment without the full reference data set.
#'
#' The purpose of the train function is to
#'
#' \enumerate{
#'   \item{Perform some munging on the data set, such as renaming
#'     columns, creating derived features, performing principal component
#'     analysis, replacing some values, removing outliers, etc.}
#'   \item{Storing the metadata necessary to replicate the munging operation
#'     after the original training set is no longer available. For example,
#'     if we are imputing a variable, we would need to remember its mean
#'     so we can use it later to replace \code{NA} values.}
#' }
#'
#' @rdname mungebit
#' @inheritParams mungebit_run
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}. Side effects on the \code{input} local variable
#'   provided to the \code{train_function} will be recorded on the mungebit
#'   object.
mungebit_train <- function(data, ...) {
  if (self$.enforce_train) {
    if (isTRUE(self$.trained)) {
      stop("This mungebit has already been trained, cannot re-train.")
    }
    on.exit(self$.trained <- TRUE, add = TRUE)
  }
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>The <code>input</code> environment used by the mungebit to record metadata from
the munging performed at training-time is the only opportunity for
affecting the <code>input</code> environment. Afterwards, we <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/bindenv.html">lock it</a>
so that we are confident the user does not modify it during prediction
time (i.e., when it is run in a real-time production system).</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  on.exit(lockEnvironment(self$.input, TRUE), add = TRUE)
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We inject the <code>input</code> helper so that the mungebit
can remember necessary metadata for replicating the
munging operation at prediction time.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  inject_metadata(self$.train_function, self$.input, self$.trained)(data, ...)
}

#' Run the predict function on a mungebit.
#'
#' The predict function is responsible for performing a munging step
#' using metadata it computed during an earlier training step.
#' This is usually done in a live production environment setting.
#'
#' The purpose of the predict function is to
#'
#' \enumerate{
#'   \item{Perform some munging on the data set, such as renaming
#'     columns, creating derived features, performing principal component
#'     analysis, replacing some values, removing outliers, etc.}
#'   \item{Use the metadata computed during the \code{train} step
#'    to correctly perform this munging.}
#' }
#'
#' @rdname mungebit
#' @inheritParams mungebit_run
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}. Side effects on the \code{input} local variable
#'   provided to the \code{predict_function} will be recorded on the mungebit
#'   object.
mungebit_predict <- function(data, ...) {
  if (!isTRUE(self$.trained)) {
    stop("This mungebit cannot predict because it has not been trained.")
  }
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We inject the <code>input</code> helper so that the mungebit
can use the metadata that was compute during training time.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  inject_metadata(self$.predict_function, self$.input, self$.trained)(data, ...)
}

inject_metadata <- function(func, input, trained) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>If there is no training or prediction function, we perform 
<em>no transformation</em> on the data or the mungebit <code>input</code>, i.e.,
we use the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/identity.html"><code>identity</code> function</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  if (is.null(func)) {
    identity
  } else {
    copy       <- func
    debug_flag <- isdebugged(func)

    environment(copy) <- list2env(list(
      input   = input,</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We also inject a helper called <code>trained</code> used for discriminating
whether the function has been trained already.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">      trained = isTRUE(trained)
    ), parent = environment(func))
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Touching a function&#39;s environment like in the expression above
<em>clears its internal debug flag</em>. We restore the flag to indicate
it is being debugged. I don&#39;t know how to detect whether a function
is <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/debug.html"><code>debugonce</code>d</a>
so if you know how to restore this flag please submit a
<a href="https://github.com/robertzk/mungebits2">pull request</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">    if (isdebugged(func)) {
      debug(copy)
    }

    copy
  }
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' @include mungebit-initialize.R mungebit-run.R mungebit-train_predict.R
NULL
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>The idea behind mungebits grew out of a year-long session 
attempting to productionize R code without translating it into
another programming language.</p>

<p>Almost every package that implements a statistical predictor
requires the user to provide a <em>wrangled</em> dataset, that is, one
stripped of outliers, with correctly coerced types, and an array
of other &ldquo;data preparation&rdquo; aspects that may affect the final
performance of the model.</p>

<p>Consider, for example, making use of a categorical variable that
has many unique values, some of which occur commonly and others
incredibly rarely. It may improve performance of some classifiers
to take the rare values, say those which occur with a frequency
of less than 5% in the data set, and label them as the value 
&ldquo;OTHER&rdquo;.</p>

<p>The choice of which variables make it into the &ldquo;OTHER&rdquo;
label is determined by the training set, which may differ across
random cross-validation splits and change as an organization 
gathers more data or the distribution shifts, such as due to
a changing consumer base or market conditions.</p>

<p>When one refits a model with the new dataset, it would be ideal if
the data preparation <em>automatically</em> reflected the updated values
by picking the set of labels that occur with greater than 5%
frequency and labeling all others as &ldquo;OTHER&rdquo;.</p>

<p>In code, we may say that</p>

<pre><code class="r">during_training &lt;- function(factor_column) {
  frequencies &lt;- table(factor_column)
  most_common &lt;- names(which(frequencies / length(factor_column) &gt; 0.05))
  factor_column &lt;- factor(
    ifelse(factor_column %in% most_common, factor_column, &quot;OTHER&quot;),
    levels = c(most_common, &quot;OTHER&quot;)
  )
  list(new_column = factor_column, most_common = most_common)
}

# Let&#39;s create an example variable.
factor_column &lt;- factor(rep(1:20, 1:20))
output &lt;- during_training(factor_column)
factor_column &lt;- output$new_column

# We would hold on to output$most_common and &quot;feed it&quot; to
# munging code that ran in production on single data points.
during_prediction &lt;- function(factor_column, most_common) {
  factor(ifelse(factor_column %in% most_common, factor_column, &quot;OTHER&quot;),
    levels = c(most_common, &quot;OTHER&quot;))
}

# Notice we have re-used our earlier code for constructing the new
# column. We will have to use the above function for munging in
# production and supply it the list `most_common` levels computed
# earlier during training.

single_data_point &lt;- 5
stopifnot(identical(
  during_prediction(5, output$most_common),
  factor(&quot;OTHER&quot;, levels = c(as.character(11:20), &quot;OTHER&quot;))
))

single_data_point &lt;- 15
stopifnot(identical(
  during_prediction(15, output$most_common),
  factor(&quot;15&quot;, levels = c(as.character(11:20), &quot;OTHER&quot;))
))

# In a real setting, we would want to operate on full data.frames
# instead of only on atomic vectors.
</code></pre>

<p>It may seem silly to create a factor variable with a single value
and a surplus of unused levels, but that is only the case if you
have never tried to productionize your data science models! Remember,
even if you trained a simple regression, your factor columns will need
to be converted to 0/1 columns using something like the <code>model.matrix</code>
helper function, and this will yell at you if the correct levels are not
there on the factor column.</p>

<p>The point of mungebits is to replace all that hard work&ndash;which in the
experience of the author has sometimes spanned data preparation procedures
composed of <em>hundreds</em> of steps like the above for collections of
<em>thousands</em> of variables&ndash;with the much simplified</p>

<pre><code class="r"># During offline training.
replace_uncommon_levels_mungebit$run(dataset)
</code></pre>

<p>The mungebit has now been &ldquo;trained&rdquo; and remembers the <code>common_levels</code>
defined earlier. In a production system, we will be able to run the
exact same code on a single row of data, as long as we serialize
the mungebit object and recall it during production. This gives us
a streaming machine learning engine that includes hard data
wrangling work&ndash;in R.</p>

<pre><code class="r"># During real-time prediction.
replace_uncommon_levels_mungebit$run(dataset)
</code></pre>

<p>After understanding mungebits, data science will stop being data
janitor work and you will get back to the math.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Construct a new mungebit.
#'
#' The majority of data projects are overcome by the burden of excessive
#' data wrangling. Part of the problem lies in the fact that when new
#' data is introduced that was drawn from the same source as the original,
#' such as a training set for a statistical model, \emph{different} code
#' needs to be written to achieve the same transformations. Mungebits solve
#' this problem by forcing the user to determine how to correctly munge
#' on out-of-sample data (such as live streaming data in the form of one-row
#' data.frames) at "munge-time", when the reason for the wrangling is still
#' apparent. A frequent source of data errors is treating this process as an
#' afterthought.
#'
#' Consider the following problem. Imagine we wish to discretize a variable,
#' say determined algorithmically with cuts [0, 0.5), [0.5, 1.5), [1.5, 3).
#' When we apply the same transformation on a new data set, we cannot run
#' the same discretization code, since it may produce new cutoffs, and hence
#' invalidate the results if, for example, we had trained a model on the
#' prior cutoffs. To ensure the exact same mathematical transformation
#' is performed on new data--whether a new test set derived from recent
#' data or a one-row data.frame representing a single record streaming
#' through a production system--we must run \emph{different code} on
#' the "original" set versus the new set.
#'
#' Mathematically speaking, a transformation of a data set can be represented
#' by a single mathematical function that is implemented differently during
#' "training" versus "prediction." Here, "training" refers to the first
#' time the transformation is performed, and "prediction" refers to 
#' subsequent times, such as on newly obtained data or a one-row data.frame
#' representing a single new record in a production system.
#'
#' Therefore, the \emph{correct} approach to data preparation, if you
#' wish to re-use it in the future on new data sets or in a live production
#' environment, is to treat it as a collection of tuples
#' \code{(train_function, predict_function, input)}, where
#' \code{train_function} represents the original code, \code{input} represents
#' an arbitrary R object such as a list, used for storing "metadata"
#' necessary to re-create the original transformation, and the
#' \code{predict_function} takes this \code{input} metadata and produces
#' the identical transformation on an out-of-sample data set.
#'
#' For example, if we wish to impute a data set, \code{train_function}
#' might compute the mean, store it in \code{input$mean}, replace
#' the \code{NA} values with the mean, and return the dataset. Meanwhile,
#' the \code{predict_function} simply replaces the \code{NA} values
#' with the cached \code{input$mean}.
#'
#' Usually, these steps would be in disjoint code bases: the modeler
#' would perform the ad-hoc munging while playing with the dataset,
#' and a software engineer would take the computed \code{input$mean}
#' and hard code it into a "data pipeline". It would be infeasible
#' to recompute the mean on-the-fly since \emph{it depends on the
#' original data set}, which may be prohibitively large. However,
#' while it may require a lot of space and time to compute the
#' original \code{input}, as they are parameterized potentially by
#' a very large data set, usually the \code{input} itself is small
#' and the resulting \code{predict_function} is inexpensive. 
#'
#' The fundamental problem of data preparation, and the reason why
#' \href{http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html}{data scientists spend over 90\% of their time on data preparation},
#' is a lack of respect for this dichotomy. Using mungebits makes
#' this duality blatantly apparent in all circumstances and will hopefully
#' reduce the amount of time wasted on cumbersome wrangling.
#'
#' @docType class
#' @format NULL
#' @name mungebit
#' @export
#' @examples
#' \dontrun{
#' mb <- mungebit(column_transformation(function(col, scale = NULL) {
#'   if (!isTRUE(trained)) { # trained is an injected keyword
#'    cat("Column scaled by ", input$scale, "\n")
#'   } else {
#'    input$scale <- scale
#'   }
#'  
#'   col * input$scale
#' }))
#' 
#' iris2 <- mb$run(iris, "Sepal.Length", 2)
#' # iris2 now contains a copy of iris with Sepal.Length doubled.
#' iris3 <- mb$run(iris2, "Sepal.Length")
#' # > Column scaled by 2
#' head(iris3[[1]] / iris[[1]])
#' # > [1] 4 4 4 4 4 4 
#' }
mungebit <- R6::R6Class("mungebit",
  public = list(
    .train_function   = NULL, # Function or NULL
    .predict_function = NULL, # Function or NULL
    .input            = NULL, # Environment
    .trained          = FALSE, # Logical
    .enforce_train    = TRUE, # Logical

    initialize = mungebit_initialize,
    run        = mungebit_run,
    train      = mungebit_train,
    predict    = mungebit_predict,

    debug      = function() { debug(self) },
    undebug    = function() { undebug(self) },
    train_function   = function() { self$.train_function   },
    predict_function = function() { self$.predict_function },
    trained    = function() { self$.trained },
    input      = function() { as.list(self$.input) }
  )
)

#' Determine whether an object is a mungebit.
#'
#' @keywords typecheck
#' @param x ANY. An R object to check.
#' @return TRUE or FALSE according as it has class mungebit.
#' @export
is.mungebit <- function(x) {
  inherits(x, "mungebit")
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungepiece-initialize.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Construct a new mungepiece.
#'
#' A mungebit defines an atomic data transformation of an \emph{arbitrary}
#' data set. In order to specify the parameters that may be relevant for
#' a \emph{particular} data set (such as restricting its effect to
#' specific columns, fixing certain parameters such as the imputation
#' method, or providing information that may contain domain knowledge)
#' one uses a mungepiece. A mungepiece defined a \emph{domain-specific}
#' atomic transformation of a data set.
#'
#' A mungepiece is defined by the collection of
#'
#' \enumerate{
#'   \item{A mungebit. }{The mungebit determines the qualitative nature
#'      of the data transformation. The mungebit may represent
#'      a discretization method, principal component analysis,
#'      replacement of outliers or special values, and so on.
#'
#'      If a training set represents automobile data and there are
#'      variables like "weight" or "make," these variables should not be
#'      hardcoded in the mungebit's \code{train} and \code{predict}
#'      functions. The mungebit should only represent that abstract
#'      \emph{mathematical} operation performed on the data set.}
#'   \item{Training arguments. }{While the mungebit represents the code
#'      necessary for performing some \emph{abstract mathematical operation}
#'      on the data set, the training arguments record the metadata
#'      necessary to perform the operation on a \emph{particular}
#'      data set.
#'
#'      For example, if we have an automobile data set and know the
#'      "weight" column has some missing values, we might pass a vector
#'      of column names that includes "weight" to an imputation mungebit
#'      and create an imputation-for-this-automobile-data mungepiece.
#'
#'      If we have a medical data set that includes special patient type
#'      codes and some of the codes were mistyped during data entry or
#'      are synonyms for the same underlying "type," we could pass a list
#'      of character vectors to a "grouper" mungebit that would condense
#'      the categorical feature by grouping like types.
#'
#'      If we know that some set of variables is predictive for modeling a
#'      particular statistical question but are unsure about the remaining
#'      variables, we could use this intuition to pass the list of known
#'      variables as exceptions to a "sure independence screening" mungebit.
#'      The mungebit would run a univariate regression against each variable
#'      not contained in the exceptions list and drop those totally uncorrelated
#'      with the dependent variable. This is a typical technique for high
#'      dimensionality reduction. Knowledge of the exceptions would reduce
#'      the computation time necessary for recording which variables are
#'      nonpredictive, an operation that may be very computationally expensive.
#'
#'      In short, the mungebit records what we are doing to the data set
#'      from an abstract level and does not contain any domain knowledge.
#'      The training arguments, the arguments passed to the mungebit's 
#'      \code{train_function}, record the details that pinpoint the
#'      abstract transformation to a \emph{particular} training set intended for
#'      use with a predictive model.}
#'   \item{Prediction arguments. }{It is important to understand the 
#'      train-predict dichotomy of the mungebit. If we are performing an
#'      imputation, the mungebit will record the means computed from the
#'      variables in the training set for the purpose of replacing \code{NA}
#'      values. The training arguments might be used for specifying the columns
#'      to which the imputation should be restricted.
#'
#'      The prediction arguments, by default the same as the training arguments,
#'      are metadata passed to the mungebit's \code{predict_function}, such as
#'      again the variables the imputation applies to. Sometimes the prediction
#'      arguments may differ slightly from the training arguments, such as when
#'      handling the dependent variable (which will not be present during
#'      prediction) or when the code used for prediction needs some further
#'      parametrization to replicate the behavior of the \code{train_function}
#'      on one-row data sets (i.e., real-time points in a production setting).}
#' } 
#'
#' In short, mungepieces \emph{parametrize} a \emph{single transformation}
#' of a data set \emph{for that particular data set}. While a mungebit is
#' abstract and domain-independent and may represent computations like 
#' imputation, outlier detection, and dimensionality reduction, a mungepiece
#' records the human touch and domain knowledge that is necessary for
#' ensuring the mungebits operate on the appropriate features and optimize
#' space-time tradeoffs (for example, the modeler may know that certain
#' columns do not contain missing values and avoid passing them to the
#' imputation mungebit).
#'
#' Informally speaking, you can think of a mungebit as the \emph{raw mold}
#' for a transformation and a mungepiece as the
#' \emph{cemented product constructed from the mold} that is specific to
#' a particular data set. A mungepiece affixes a mungebit so it works on
#' a specific data set, and domain knowledge may be necessary to construct
#' the mungepiece optimally.
#'
#' @param mungebit mungebit. A mungebit \code{\link{mungebit}} representing
#'    an abstract transformation of a data set, such as type conversion,
#'    imputation, outlier detection, dimensionality reduction,
#'    value replacement, and so on.
#' @param train_args list. Arguments to pass to the mungebit when it is
#'    run for the first time, i.e., on a \emph{training set} that will be
#'    fed to a predictive model and may be quite large. These arguments,
#'    passed directly to the mungebit's \code{train_function}, should 
#'    contain domain-specific metadata that is necessary to apply the
#'    mungebit to this specific data set.
#'
#'    For example, if the modeler knows certain columns do not contain 
#'    missing values, they might pass a character vector of column names
#'    to an imputation mungebit that avoids attempting to impute the
#'    columns guaranteed to be fully present. Doing this heuristically might
#'    require an unnecessary pass over the data, potentially expensive if
#'    the data consists of thousands of features; domain-specific knowledge
#'    might be used to pinpoint the few features that require imputation.
#' @param predict_args list. Arguments to pass to the mungebit when it
#'    is run for the second or subsequent time, i.e., on a \code{prediction set}
#'    that will usually be coming from model validation code or a real-time
#'    production environment. After the mungebit has been trained on the
#'    training set, it should be capable of predicting on
#'    \emph{single row data sets}, i.e., new "points" coming through in
#'    a live production setting.
#'
#'    Usually, the prediction arguments will be the same as the training
#'    arguments for the mungepiece.
#' @examples
#' \dontrun{
#'   doubler <- mungebit$new(column_transformation(function(x) x * 2))
#'   cols    <- c("Sepal.Length", "Petal.Length")
#'   mp      <- mungepiece$new(doubler, list(cols))
#'   iris2   <- mp$run(iris)
#'   stopifnot(identical(iris2[cols], 2 * iris[cols]))
#' }
mungepiece_initialize <- function(mungebit     = NULL,
                                  train_args   = list(),
                                  predict_args = train_args) {

  if (!is.mungebit(mungebit)) {
    stop("To create a new mungepiece, please pass a ",
         sQuote("mungebit"), " as the first argument. I received ",
         "something of class ", sQuote(crayon::red(class(mungebit)[1L])), ".")
  }

  if (!is.list(train_args)) {
    stop("To create a new mungepiece, please pass a list (of training ",
         "arguments) as the second argument. I received something of ",
         "class ", sQuote(crayon::red(class(train_args)[1L])), ".")
  }

  if (!is.list(predict_args)) {
    stop("To create a new mungepiece, please pass a list (of prediction ",
         "arguments) as the third argument. I received something of ",
         "class ", sQuote(crayon::red(class(predict_args)[1L])), ".")
  }

  self$.mungebit     <- mungebit
  self$.train_args   <- make_env(train_args)
  self$.predict_args <- make_env(predict_args)

  lockEnvironment(self$.train_args,   bindings = TRUE)
  lockEnvironment(self$.predict_args, bindings = TRUE)
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungepiece-run.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Running a mungepiece respects the same laws as running a mungebit.
During training, the goal is to record the necessary metadata the
mungebit needs in order to run during prediction (i.e., on one
row data sets in a real-time production environment).</p>

<p>The first time <code>mungepiece$run</code> is called, the call is delegated
to the <code>mungebit</code> attached to the mungepiece with the appropriate
training arguments. </p>

<p>For example, imagine we have a mungebit that discretizes a variable.</p>

<pre><code class="r">discretizer_train &lt;- function(data, columns, breaks = 5) {
  # Recall that the first argument to a mungebit&#39;s train function
  # is *always* the data set. The additional arguments, in this 
  # case the column names to discretize, will be the list of
  # training arguments on the mungepiece.
  stopifnot(is.character(columns), all(columns %in% colnames(data)))

  # We record the columns that were discretized.
  input$`_columns` &lt;- columns

  for (column in columns) {
    # Record the values to discretize at, i.e., the bounds of each interval.
    quantiles &lt;- quantile(data[[column]], breaks = breaks)
    # `cuts` will be the discretized variable using R&#39;s `base::cut`.
    cuts &lt;- cut(data[[column]], breaks = quantiles)
    # We need to remember the cut points and levels to discretize during 
    # prediction correctly.
    input[[column]] &lt;- list(cuts = quantiles, levels = levels(cuts)) 
    # We assume there are no missing values.
    data[[column]]  &lt;- cuts
  }

  data
}

# This function will be pretty slow in R. You can rewrite it in Rcpp.
# It also suffers from a few bugs on the boundaries due to open/closed
# interval issues, but a full implementation defeats the illustration.
discretizer_predict &lt;- function(data, columns, ...) {
  # We leave the last argument as ... in case the user left the train 
  # arguments the same as the predict arguments so that we may absorb
  # the `breaks` argument without error.
  if (missing(columns)) columns &lt;- input$`_columns`

  # We only allow columns that were discretized during training and are
  # present in the dataset. A more strict approach would throw an error.
  columns &lt;- intersect(intersect(columns, input$`_columns`), colnames(data))
  # Some helper functions.
  coalesce &lt;- function(x, y) { if (length(x) == 0) y[1L] else x[1L] }
  min2     &lt;- function(x) { if (length(x) == 0) NULL else min(x) }

  for (column in columns) {
    cuts &lt;- vapply(data[[column]], function(value) {
      # Convince yourself that `ix` will be the index of the correct
      # label. For example, if value is `2.5` and `levels` are [0, 1],
      # (1, 2], (2, 3], (3, 4], then `ix` will be 3.
      ix &lt;- max(1, coalesce(
        min2(which(c(-Inf, input[[column]]$cuts[-1L]) &gt;= value)),
        length(input[[column]]$levels) + 1
       ) - 1)
      input[[column]]$levels[ix]
    }, character(1))
    data[[column]] &lt;- factor(cuts, levels = input[[column]]$levels)
  }

  data   
}

bit &lt;- mungebit$new(discretizer_train, discretizer_predict)
</code></pre>

<p>Note that the code to implement discretization during training and 
prediction is quite different! We can turn this mungebit into a 
mungepiece that operates on the <code>iris</code> dataset.</p>

<pre><code class="r">piece &lt;- mungepiece$new(bit, list(c(&quot;Sepal.Width&quot;, &quot;Sepal.Length&quot;)))
iris2 &lt;- mungepiece$run(iris) # Train the mungepiece.
head(iris2$Sepal.Length)
# [1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (5.1,5.8]
# Levels: (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9]
iris3 &lt;- piece$run(iris[1:6, ]) # It has been trained and run live.
print(iris3$Sepal.Length)
# [1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (4.3,5.1] (5.1,5.8]
# Levels: (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] 
stopifnot(identical(head(iris2$Sepal.Length), iris3$Sepal.Length))
</code></pre>

<p>The mungepiece also works correctly on outliers.</p>

<pre><code class="r">irisc &lt;- iris; irisc[1:2, 1] &lt;- c(0, 10)
print(piece$run(irisc[1:2, ])$Sepal.Length)
# [1] (4.3,5.1] (6.4,7.9]
# Levels: (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9]
</code></pre>

<p>It is important to handle such cases if new points in a live production
setting have values that are outside the observed range of the training
set.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Run a mungepiece and prepare it for a live production setting.
#'
#' Running a mungepiece achieves the same effect as running the mungebit
#' attached to the mungepiece: the first time it is run, we \emph{train}
#' the mungebit so it remembers metadata it will need to replicate the
#' operation in a live production setting on a single row of data. The
#' second and subsequent times we run the mungepiece, it will execute
#' the predict function of the underlying mungebit.
#'
#' @inheritParams mungebit_run
#' @param _envir environment. The calling environment for the train
#'    or predict function on the underlying mungebit. This is an internal
#'    argument and is \code{parent.frame()} by default.
#' @return If the \code{data} parameter is an environment, the transformed
#'    environment (i.e., the transformed data in the environment) after 
#'    application of the underlying mungebit. If \code{data} is a data.frame,
#'    the transformed data.frame is returned.
mungepiece_run <- function(data, ..., `_envir` = parent.frame()) {
  if (self$.mungebit$trained()) {
    calling_environment <- self$.predict_args
    reference_function  <- self$.mungebit$predict_function()
  } else {
    calling_environment <- self$.train_args
    reference_function  <- self$.mungebit$train_function()
  }

  args <- eval(substitute(alist(...)))
  args <- two_way_argument_merge(strip_arguments(reference_function, 1),
                                 calling_environment, args)

  parent.env(calling_environment) <- `_envir`
  on.exit(parent.env(calling_environment) <- emptyenv(), add = TRUE)

  args <- c(list(data = substitute(data)), args)

  do.call(self$.mungebit$run, args, envir = calling_environment)
}

strip_arguments <- function(fun, n) {
  formals(fun) <- formals(fun)[setdiff(seq_along(formals(fun)), seq_len(n))]
  fun
}

two_way_argument_merge <- function(reference_function, calling_environment, args) {
  call      <- as.call(c(alist(self), args))
  base_args <- as.list(match.call(reference_function, call)[-1L])

  default_args <- env2listcall(calling_environment)
  names(default_args) <- attr(calling_environment, "initial_names")
  call         <- as.call(c(alist(self), default_args))
  default_args <- as.list(match.call(reference_function, call)[-1L])

  if (unnamed_count(default_args) > 0 && unnamed_count(base_args) > 0) {
    default_args[unnamed(default_args)] <- NULL
  }

  list_merge(default_args, base_args)
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungepiece.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' @include mungepiece-initialize.R mungepiece-run.R
NULL
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Mungebits are intended to record the dichotomy between computations
that occur at training time, such as computing the means of variables
during imputation, and prediction time, such as restoring <code>NA</code> values
with the precomputed means at prediction time.</p>

<p>While a mungebit records the general computation that can apply to 
arbitrary datasets, a <em>mungepiece</em> records the training and prediction
arguments applicable to the mungebit. For example, if we used an imputation
mungebit that looked as follows</p>

<pre><code class="r">imputation_mungebit &lt;- mungebit$new(function(data, columns) {
  data[columns] &lt;- lapply(columns, function(column) {
    if (isTRUE(trained)) {
      input[[column]] &lt;- mean(data[[column]], na.rm = TRUE)
    }
    ifelse(is.na(data[[column]]), input[[column]], data[[column]])
  })
})
</code></pre>

<p>then we may wish to record the columns to which the imputation 
applies. In this case, we can use a <em>mungepiece</em>.</p>

<pre><code class="r">piece &lt;- mungepiece$new(imputation_mungebit, imputed_columns, imputed_columns)
</code></pre>

<p>To run the mungepiece on our data set we can say <code>piece$run(data, column_names)</code>. 
The advantage of this approach is that after the mungepiece has been trained,
it will remember the means and can be used on single row data.frames (i.e.,
those coming in from production) without a change in syntax or calling
convention. This means that the typical disproportion of spending the
majority of one&#39;s time &ldquo;munging data&rdquo; is drastically reduced and no 
further code has to be written to ensure the transformations run correctly
in a production setting.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">mungepiece <- R6::R6Class("mungepiece", 
  public = list(
    .mungebit     = NULL, # mungebit
    .train_args   = NULL, # list
    .predict_args = NULL,

    initialize = mungepiece_initialize,
    run        = mungepiece_run,

    debug      = function() { debug(self$.mungebit) },
    undebug    = function() { undebug(self$.mungebit) },
    trained    = function() { self$.mungebit$trained() },
    mungebit   = function() { self$.mungebit }
  )
)

#' Determine whether an object is a mungepiece.
#'
#' @keywords typecheck
#' @param x ANY. An R object to check.
#' @return TRUE or FALSE according as it has class mungepiece
#' @export
is.mungepiece <- function(x) {
  inherits(x, "mungepiece")
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>package.mungebits2.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' An approach to data preparation that is compatible with production systems.
#'
#' Mungebits2 defines a way of thinking about data preparation that
#' couples the definition of what happens in batch processing versus
#' online prediction so that both can be described by the same codebase.
#'
#' For example, consider the simple example of imputation. While the
#' general concept of imputing a variable works on arbitrary codebases,
#' a \emph{separate} data transformation will have to be defined for
#' each model that uses imputation in a production setting. This is 
#' because the imputed value depends inherently on the dataset.
#' We must remember the mean of the data set encountered during
#' training, and recall this value when performing replacement in
#' a production setting.
#'
#' Mungebits provide a sort of "train track switch" that allows one
#' to write data preparation offline, but ensure it works online 
#' (on a stream of new data, such as one-row data.frames).
#'
#' By reframing data preparation as the process of constructing
#' a "munge procedure", a list of trained mungebits that can
#' reproduce the same mathematical operation on a dataset in
#' a production environment without additional code, the process
#' of productionizing a machine learning model should become
#' significantly simplified.
#'
#' @name mungebits2
#' @import crayon lazyeval R6 stagerunner
#' @docType package
NULL

utils::globalVariables("self")
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>parse_mungepiece.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Constructing a mungepiece is not incredibly straightforward. First,
we must construct the mungebit, which represents the code that will
be executed when running the mungepiece on a training dataset to
later feed to a machine learning classifier (i.e., the train function)
in conjunction with the code that executes on streaming records
coming through in a production system performing the same
mathematical operation on a 1-row dataset (i.e., the predict function).</p>

<p>Next, we must determine the training and prediction arguments to the
mungebit that specify the difference in how to use the mungebit on
offline training versus realtime prediction data.</p>

<p>Thus, constructing a mungepiece looks something like:</p>

<pre><code class="r">piece &lt;- mungepiece(
  mungebit(train_function, predict_function),
  train_args, predict_args
)
</code></pre>

<p>In particular, we have to invoke the <code>mungebit</code> constructor every time
we create a mungepiece. Instead the <code>parse_mungepiece</code> helper defined in
this file provides a more convenient format:</p>

<pre><code class="r"># If the train function with train args is the same as the predict function
# with predict args.
piece &lt;- parse_mungepiece(list(train_fn, train_arg1, train_arg2 = &quot;blah&quot;))

# If the train and predict arguments to the mungepiece match, but we
# wish to use a different train versus predict function for the mungebit.
piece &lt;- parse_mungepiece(list(train_fn, predict_fn), dual_arg1, dual_arg2 = &quot;blah&quot;)

# If we wish to only run this mungepiece during training.
piece &lt;- parse_mungepiece(list(train_fn, NULL), train_arg1, train_arg2 = &quot;blah&quot;)

# If we wish to only run this mungepiece during prediction
piece &lt;- parse_mungepiece(list(NULL, predict_fn), predict_arg1, predict_arg2 = &quot;blah&quot;)

# If we wish to run different arguments but the same function during
# training versus prediction.
piece &lt;- parse_mungepiece(train = list(train_fn, train_arg1),
                          predict = list(train_fn, predict_arg1))

# If we wish to run different arguments with different functions during
# training versus prediction.
piece &lt;- parse_mungepiece(train = list(train_fn, train_arg1),
                          predict = list(predict_fn, predict_arg1))
</code></pre>

<p>This is a full partition of the potential arguments used to initialize a
mungebit + mungepiece combo. Using this syntax in conjunction with the
<code>munge</code> helper function speeds up coding of munge procedures (lists of
mungepieces) and increases the readability of munging code.</p>

<pre><code class="r"># The munge function calls out to the parse_mungepiece helper.
munged_data &lt;- munge(raw_data, list(
  &quot;Drop useless vars&quot; = list(list(drop_vars, vector_of_variables),
                             list(drop_vars, c(vector_variables, &quot;dep_var&quot;))),
  &quot;Impute variables&quot;  = list(imputer, imputed_vars),
  &quot;Discretize vars&quot;   = list(list(discretize, restore_levels), discretized_vars)
))
</code></pre>

<p>Translated in English, we are saying:</p>

<ol>
<li><p>Drop a static list of useless variables from the data set.
 When the model is trained, drop the dependent variable as well
 since we will no longer need it.</p></li>
<li><p>Impute the variables in the static list of <code>imputed_vars</code>.
 When the model is trained, the <code>imputer</code> will have some logic
 to restore the means obtained during training of the mungepiece.</p></li>
<li><p>Discretize the static list of variables in the <code>discretized_vars</code>
 character vector. After model training, when new data points come in,
 the original training set is no longer available. The <code>discretize</code>
 method stored the necessary cuts for each variable in the mungebit&#39;s
 <code>input</code>, which the <code>restore_levels</code> function uses to bin the
 numerical features in the list of <code>discretized_vars</code> into factor
 (categorical) variables.</p></li>
</ol>

<p>If one took an initial (training) data set, ran it through the
<code>munge</code> helper as above, took the resulting list of mungepieces,
and ran the original data set through them a second time (so they
are running in &ldquo;predict mode&rdquo;), we should obtain the same result.</p>

<p>We can use the list of trained mungepieces to replicate the munging
on the training set in a production system on single row data sets
(i.e., new records being streamed in real-time).</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Translate a list of train / predict functions and arguments to a mungepiece.
#'
#' Constructing mungepieces and mungebits by hand is a little tedious.
#' To simplify the process, we introduce a tiny DSL that allows for
#' easier construction of mungebits. The intention is for this function
#' to be used in conjuction with a list passed to the \code{\link{munge}}
#' helper.
#'
#' @note To understand the documentation of this helper, please read
#'   the documentation on \code{\link{mungebit}} and \code{\link{mungepiece}}
#'   first.
#' @param args list. A list of arguments. This can be one of the following formats
#'   
#'   \enumerate{
#'     \item{\code{list(train_fn, ...)}}{ -- If the first element of \code{args} is
#'       a function followed by other arguments, the constructed mungepiece
#'       will use the \code{train_fn} as both the \emph{train and predict}
#'       function for the mungebit, and \code{list(...)} (that is, the remaining 
#'       elements in the list) will be used as both the train and predict
#'       arguments in the mungepiece. In other words, using this format
#'       specifies you would like \emph{exactly the same behavior in
#'       training as in prediction}. This is appropriate for mungebits
#'       that operate in place and do not need information obtained
#'       from the training set, such as simple value replacement or
#'       column removal.
#'     }
#'     \item{\code{list(list(train_fn, predict_fn), ...)}}{
#'       -- If \code{args} consists of a two-element pair in its first
#'       element, it must be a pair of either \code{NULL}s or functions,
#'       with not both elements \code{NULL}. If the \code{train_fn}
#'       or \code{predict_fn}, resp., is \code{NULL}, this will signify to have
#'       \emph{no effect} during training or prediction, resp.
#'        
#'       The remaining arguments, that is \code{list(...)}, will be used
#'       as both the training and prediction arguments.
#'   
#'       This structure is ideal if the behavior during training and prediction
#'       has an identical parametrization but very different implementation,
#'       such as imputation, so you can pass two different functions.
#'
#'       It is also useful if you wish to have no effect during prediction,
#'       such as removing faulty rows during training, or no effect during
#'       training, such as making a few transformations that are only
#'       necessary on raw production data rather than the training data.
#'     }
#'     \item{\code{list(train = list(train_fn, ...), predict = list(predict_fn, ...))}}{
#'       If \code{args} consists of a list consisting of exactly two named
#'       elements with names "train" and "predict", then the first format will be
#'       used for the respective fields. In other words, a mungepiece will
#'       be constructed consisting of a mungebit with \code{train_fn} as the
#'       training function, \code{predict_fn} as the predict fuction, and
#'       the mungepiece train arguments will be the train list of additional
#'       arguments \code{list(...)}, and similarly the predict arguments will be
#'       the predict list of additional arguments \code{list(...)}.
#'  
#'       Note \code{train_fn} and \code{predict_fn} must \emph{both} be functions
#'       and not \code{NULL}, since then we could simply use the second format
#'       described above.
#'
#'       This format is ideal when the parametrization differs during training and
#'       prediction. In this case, \code{train_fn} usually should be the same
#'       as \code{predict_fn}, but the additional arguments in each list can
#'       be used to identify the parametrized discrepancies. For example, to
#'       sanitize a dataset one may wish to drop unnecessary variables. During
#'       training, this excludes the dependent variable, but during prediction
#'       we may wish to drop the dependent as well.
#'
#'       This format can also be used to perform totally different behavior on
#'       the dataset during training and prediction (different functions and
#'       parameters), but mungebits should by definition achieve the same
#'       operation during training and prediction, so this use case is rare
#'       and should be handled carefully.
#'     }
#'   }
#'
#'   Note that the above trichotomy is exhaustive: any mungepiece can be
#'   constructed using this helper, regardless of its mungebit's
#'   train or predict function or its own train or predict arguments.
#'   In the first two formats, the first unnamed list element is always
#'   reserved and will never belong to the \code{train_args} or \code{predict_args}
#'   of the mungepiece.
#'
#'   Also note that in the first two formats, the first list element must be
#'   unnamed.
#' @return The constructed \code{\link{mungepiece}}.
#' @seealso \code{\link{mungepiece}}, \code{\link{mungebit}}.
#' @examples
#' # TODO: (RK) Fill out some examples
#' TRUE
parse_mungepiece <- function(args) {

}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>utils.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Merge two lists and overwrite latter entries with former entries
#' if names are the same.
#'
#' For example, \code{list_merge(list(a = 1, b = 2), list(b = 3, c = 4))}
#' will be \code{list(a = 1, b = 3, c = 4)}.
#' @param list1 list
#' @param list2 list
#' @return the merged list.
#' @export
#' @examples
#' stopifnot(identical(list_merge(list(a = 1, b = 2), list(b = 3, c = 4)),
#'                     list(a = 1, b = 3, c = 4)))
#' stopifnot(identical(list_merge(NULL, list(a = 1)), list(a = 1)))
list_merge <- function(list1, list2) {
  list1 <- list1 %||% list()
  # Pre-allocate memory to make this slightly faster.
  list1[Filter(function(x) nchar(x) > 0, names(list2) %||% c())] <- NULL
  for (i in seq_along(list2)) {
    name <- names(list2)[i]
    if (!identical(name, NULL) && !identical(name, "")) list1[[name]] <- list2[[i]]
    else list1 <- append(list1, list(list2[[i]]))
  }
  list1
}

`%||%` <- function(x, y) if (is.null(x)) y else x

is.acceptable_function <- function(x) {
  is.function(x) || 
  is.null(x)     ||
  is.mungebit(x)
}

# If an environment contains variables "a" and "b",
# create a list (a = quote(a), b = quote(b)).
env2listcall <- function(env) {
  names <- ls(env)
  if ("name_order" %in% names(attributes(env))) {
    names <- names[attr(env, "name_order")]
  }
  setNames(lapply(names, as.name), nm = names)
}

make_env <- function(lst, parent = emptyenv()) {
  initial_names <- names(lst) %||% character(length(lst))
  names(lst) <- ifelse(unnamed(lst),
    paste0("_", seq_along(lst)),
    paste0("_", initial_names)
  )

  if (length(lst) == 0) {
    env <- new.env(parent = parent)
  } else {
    env <- list2env(lst, parent = parent)
  }

  name_order <- match(names(lst), ls(env))
  attr(env, "name_order")    <- name_order
  attr(env, "initial_names") <- initial_names
  env
}

unnamed <- function(el) {
  "" == (names(el) %||% character(length(el)))
}

unnamed_count <- function(el) {
  sum(unnamed(el))
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>zzz.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"># TODO: (RK) Add notice if mungebits and mungebits2 are
# loaded at the same time.

.onLoad <- function(libPath, pkg) {
  if (as.package_version(R.version) < as.package_version("3.1.0")) {
    packageStartupMessage(crayon::red(paste0(
      "Using mungebits2 with R version < 3.1 will result ",
      "in dramatic performance slowdowns."
    )))
  }
}
</span></code>
            </pre>
          </div>
        </div>
      <div class="section">
      </div>

    </div>
  </body>
</html>
