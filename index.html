<!DOCTYPE html>
<html lang="en" class="">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="description" content="A way of thinking about data preparation that couples the definition of what happens in batch processing versus online prediction so that both can be described by the same codebase. With mungebits, you can save time on having to re-implement your R code to work in production and instead re-use the same codebase.">

    <title>mungebits2 (http://github.com/robertzk/mungebits2)</title>

    <link rel="stylesheet" media="all" href="stylesheets/rocco.css" />
    <link rel="stylesheet" media="all" href="stylesheets/github-markdown.css" />

    <script src="assets/highlight.pack.js"></script>
    <script type="text/javascript">
      hljs.initHighlightingOnLoad();
    </script>

    <style type="text/css">
      .header {
        position: fixed;
        top: 0px;
        width: 100%;
        background-color: rgba(0, 0, 0, 0.25);
        padding: 10px;
      }
      
      .header a {
        padding-right: 30px;
      }

      .container {
        margin-top: 40px;
      }

      body {
        padding: 0;
        margin: 0;
      }

      div.code-background {
        float: right;
        position: fixed;
        z-index: -1;
        height: 100%;
        background-color: #f8f8ff;
        width: 60%;
        right: 0px;
      }

      div.section {
        clear: both;
        margin: 0; padding: 0;
      }

      div.code {
        float: right;
        width: 60%;
      }

      code.R {
        font-size: 1.2em;
        line-height: 2em;
        margin-top: 0em;
        margin-bottom: -2em;
        padding-top: 0;
        margin-top: -1em;
      }

      code.R > span.spacer {
        position: relative;
      }

      div.code > pre {
        margin: 0;
        padding-left: 2em;
        margin-top: 0;
        margin-bottom: 0;
      }

      div.markdown {
        padding: 1em;
        padding-top: 0;
        background: #fff;
        float: left;
        width: 35%;
      }
    </style>

  </head>

  <body>
    <div class="header">
      <a href="https://github.com/robertzk/rocco">
        <img id="rocco-logo" src="https://img.shields.io/badge/Generated by rocco_v0.1.1-%E2%9C%93-blue.svg"/>
      </a>
    </div>
    <div class="container">

      <div class="code-background"></div>

        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-debug.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Debugging the train and predict function of a mungebit should be
transparent to the user. Say we have a mungebit called <code>value_replacer</code>.
By calling <code>debug(value_replacer)</code>, we should be able to simultaneously
set debug hooks on both the <code>train_function</code> and <code>predict_function</code>
of the mungebit. Calling <code>undebug(value_replacer)</code> will remove the hooks.</p>

<p>R has a tremendous array of debugging tools. You should familiarize
yourself with them to make your life much simpler. A great resource
is chapter 8 of <a href="http://www.burns-stat.com/pages/Tutor/R_inferno.pdf">The R Inferno</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Generic debugging.
#'
#' @inheritParams base::debug
#' @seealso \code{\link[base]{debug}}
#' @export
debug <- function(fun, text = "", condition = NULL) {
  UseMethod("debug")
}

#' @export
debug.default <- base::debug

#' @export 
debug.mungebit <- function(fun, text = "", condition = NULL) {
  for (fn in list(fun$.train_function, fun$.predict_function)) {
    if (is.function(fn)) {
      debug(fn, text, condition)
    }
  }
}

#' Generic removal of debugging.
#'
#' @inheritParams base::undebug
#' @seealso \code{\link[base]{undebug}}
#' @export
undebug <- function(fun) {
  UseMethod("undebug")
}

#' @export
undebug.default <- base::undebug

#' @export 
undebug.mungebit <- function(fun) {
  for (fn in list(fun$.train_function, fun$.predict_function)) {
    if (is.function(fn) && isdebugged(fn)) {
      undebug(fn)
    }
  }
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-initialize.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Constructor for mungebit class.
#'
#' Mungebits are atomic data transformations of a data.frame that,
#' loosely speaking, aim to modify "one thing" about a variable or
#' collection of variables. This is pretty loosely defined, but examples
#' include dropping variables, mapping values, discretization, etc.
#'
#' @param train_function function. This specifies the behavior to perform
#'    on the dataset when preparing for model training. A value of NULL
#'    specifies that there should be no training step, i.e., the data
#'    should remain untouched.
#' @param predict_function function. This specifies the behavior to perform
#'    on the dataset when preparing for model prediction. A value of NULL
#'    specifies that there should be no prediction step, i.e., the data
#'    should remain untouched.
#' @param enforce_train logical. Whether or not to flip the trained flag
#'    during runtime. Set this to FALSE if you are experimenting with
#'    or debugging the mungebit.
#' @examples
#' mb <- mungebit(column_transformation(function(column, scale = NULL) {
#'   # `trained` is a helper provided by mungebits indicating TRUE or FALSE
#'   # according as the mungebit has been run on a dataset.
#'   if (!trained) {
#'     cat("Column scaled by ", input$scale, "\n")
#'   } else {
#'     # `input` is a helper provided by mungebits. We remember the
#'     # the `scale` so we can re-use it during prediction.
#'     input$scale <- scale
#'   }
#'   column * input$scale
#' }))
#' 
#' # A `mungeplane` is just a lightweight wrapper to keep track of our data so
#' # the mungebit can perform side effects (i.e., modify the data without an
#' # explicit assignment <- operator).
#' irisp <- mungeplane(iris)
#' mb$run(irisp, 'Sepal.Length', 2)
#'
#' head(mp$data[[1]] / iris[[1]])
#' # > [1] 2 2 2 2 2 2
#' mb$run(mp, 'Sepal.Length')
#' # > Column scaled by 2
#' head(mp$data[[1]] / iris[[1]])
#' # > [1] 4 4 4 4 4 4 
initialize <- function(train_function   = base::identity,
                       predict_function = train_function,
                       enforce_train    = TRUE) {

  # TODO: (RK) Sanity checks?
  self$.train_function   <- train_function
  self$.predict_function <- predict_function
  self$.input            <- new.env(parent = emptyenv())
  self$.trained          <- FALSE
  self$.enforce_train    <- enforce_train
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-run.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Imagine running an imputation script on a dataset. 
On a training set, we have to compute the mean and replace
the <code>NA</code>s with its value. However, when a single row comes
in through a streaming production system, we merely need to
memorize the computed mean and replace a variable with it
if it is <code>NA</code>.</p>

<p>Calling the <code>run</code> method on a mungebit will store any
data it needs for production, such as imputed means,
in the <code>input</code> member. The second time <code>$run</code> is called
(i.e., during prediction or real-time production use),
it will be using the <code>predict_function</code> rather than the
<code>train_function</code>, which will be less computationally expensive
since it does not have to operate in reference to a training set
and can use the memorized results in <code>input</code> to achieve
the same transformation as the <code>train_function</code>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Run a mungebit.
#' 
#' Imagine flipping a switch on a set of train tracks. A mungebit
#' behaves like this: once the \code{trained} switch is flipped,
#' it can only run the \code{predict_function}, otherwise it will
#' run the \code{train_function}.
#'
#' @rdname mungebit
#' @param data environment or data.frame. Essentially an environment
#'   containing a \code{data} variable. In this case, that \code{data} variable
#'   will have a side effect enacted on it. If a \code{data.frame}, then 
#'   the return value will be the modified \code{data.frame} and the mungebit
#'   will record any results it must memorize in its \code{input}.
#' @param ... additional arguments to the mungebit's \code{train_function} or
#'   \code{predict_function}.
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}.
mungebit_run <- function(data, ...) {
  if (is.environment(data)) {
    if (!exists("data", envir = data, inherits = FALSE)) {
      stop("If you are passing an environment to a mungebit, you must ",
           "provide one that contains a ", sQuote("data"), " key.")
    }

    data$data <- self$run(data$data, ...)
  } else if (isTRUE(self$.trained)) {
    data <- self$predict(data, ...)
  } else {
    data <- self$train(data, ...)  
  }
  data
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit-train_predict.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Run the train function on a mungebit.
#'
#' The train function is responsible for performing a munging step and
#' storing metadata that can replicate the munging step in a live
#' production environment without the full reference data set.
#'
#' The purpose of the train function is to
#'
#' \enumerate{
#'   \item{Perform some munging on the data set, such as renaming
#'     columns, creating derived features, performing principal component
#'     analysis, replacing some values, removing outliers, etc.}
#'   \item{Storing the metadata necessary to replicate the munging operation
#'     after the original training set is no longer available. For example,
#'     if we are imputing a variable, we would need to remember its mean
#'     so we can use it later to replace \code{NA} values.}
#' }
#'
#' @rdname mungebit
#' @inheritParams mungebit_run
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}. Side effects on the \code{input} local variable
#'   provided to the \code{train_function} will be recorded on the mungebit
#'   object.
mungebit_train <- function(data, ...) {
  if (enforce_train) {
    on.exit(self$.trained <- TRUE, add = TRUE)
  }
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>The <code>input</code> environment used by the mungebit to record metadata from
the munging performed at training-time is the only opportunity for
affecting the <code>input</code> environment. Afterwards, we <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/bindenv.html">lock it</a>
so that we are confident the user does not modify it during prediction
time (i.e., when it is run in a real-time production system).</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  on.exit(lockEnvironment(self$.input), add = TRUE)
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We inject the <code>input</code> helper so that the mungebit
can remember necessary metadata for replicating the
munging operation at prediction time.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  inject_metadata(self.$train_function, self$.input, self$.trained)(data, ...)
}

#' Run the predict function on a mungebit.
#'
#' The predict function is responsible for performing a munging step
#' using metadata it computed during an earlier training step.
#' This is usually done in a live production environment setting.
#'
#' The purpose of the predict function is to
#'
#' \enumerate{
#'   \item{Perform some munging on the data set, such as renaming
#'     columns, creating derived features, performing principal component
#'     analysis, replacing some values, removing outliers, etc.}
#'   \item{Use the metadata computed during the \code{train} step
#'    to correctly perform this munging.}
#' }
#'
#' @rdname mungebit
#' @inheritParams mungebit_run
#' @return The modified \code{data}, whether it is an \code{environment}
#'   or \code{data.frame}. Side effects on the \code{input} local variable
#'   provided to the \code{predict_function} will be recorded on the mungebit
#'   object.
mungebit_predict <- function(data, ...) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We inject the <code>input</code> helper so that the mungebit
can use the metadata that was compute during training time.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  inject_metadata(self.$predict_function, self$.input, self$.trained)(data, ...)
}

inject_metadata <- function(func, input, trained) {</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>If there is no training or prediction function, we perform 
<em>no transformation</em> on the data or the mungebit <code>input</code>, i.e.,
we use the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/identity.html"><code>identity</code> function</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">  if (is.null(func)) {
    identity
  } else {
    copy       <- func
    debug_flag <- isdebugged(func)

    environment(copy) <- list2env(list(
      input   = input,</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>We also inject a helper called <code>trained</code> used for discriminating
whether the function has been trained already.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">      trained = isTRUE(trained)
    ), parent = environment(func))
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>Touching a function&#39;s environment like in the expression above
<em>clears its internal debug flag</em>. We restore the flag to indicate
it is being debugged. I don&#39;t know how to detect whether a function
is <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/debug.html"><code>debugonce</code>d</a>
so if you know how to restore this flag please submit a
<a href="https://github.com/robertzk/mungebits2">pull request</a>.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">    if (isdebugged(func)) {
      debug(copy)
    }

    copy
  }
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>mungebit.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' @include mungebit-initialize.R
run <- train <- predict <- function(...) { }
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <p>The idea behind mungebits grew out of a year-long session 
attempting to productionize R code without translating it into
another programming language.</p>

<p>Almost every package that implements a statistical predictor
requires the user to provide a <em>wrangled</em> dataset, that is, one
stripped of outliers, with correctly coerced types, and an array
of other &ldquo;data preparation&rdquo; aspects that may affect the final
performance of the model.</p>

<p>Consider, for example, making use of a categorical variable that
has many unique values, some of which occur commonly and others
incredibly rarely. It may improve performance of some classifiers
to take the rare values, say those which occur with a frequency
of less than 5% in the data set, and label them as the value 
&ldquo;OTHER&rdquo;.</p>

<p>The choice of which variables make it into the &ldquo;OTHER&rdquo;
label is determined by the training set, which may differ across
random cross-validation splits and change as an organization 
gathers more data or the distribution shifts, such as due to
a changing consumer base or market conditions.</p>

<p>When one refits a model with the new dataset, it would be ideal if
the data preparation <em>automatically</em> reflected the updated values
by picking the set of labels that occur with greater than 5%
frequency and labeling all others as &ldquo;OTHER&rdquo;.</p>

<p>In code, we may say that</p>

<pre><code class="r">during_training &lt;- function(factor_column) {
  frequencies &lt;- table(factor_column)
  most_common &lt;- names(which(frequencies / length(factor_column) &gt; 0.05))
  factor_column &lt;- factor(
    ifelse(factor_column %in% most_common, factor_column, &quot;OTHER&quot;),
    levels = c(most_common, &quot;OTHER&quot;)
  )
  list(new_column = factor_column, most_common = most_common)
}

# Let&#39;s create an example variable.
factor_column &lt;- factor(rep(1:20, 1:20))
output &lt;- during_training(factor_column)
factor_column &lt;- output$new_column

# We would hold on to output$most_common and &quot;feed it&quot; to
# munging code that ran in production on single data points.
during_prediction &lt;- function(factor_column, most_common) {
  factor(ifelse(factor_column %in% most_common, factor_column, &quot;OTHER&quot;),
    levels = c(most_common, &quot;OTHER&quot;))
}

# Notice we have re-used our earlier code for constructing the new
# column. We will have to use the above function for munging in
# production and supply it the list `most_common` levels computed
# earlier during training.

single_data_point &lt;- 5
stopifnot(identical(
  during_prediction(5, output$most_common),
  factor(&quot;OTHER&quot;, levels = c(as.character(11:20), &quot;OTHER&quot;))
))

single_data_point &lt;- 15
stopifnot(identical(
  during_prediction(15, output$most_common),
  factor(&quot;15&quot;, levels = c(as.character(11:20), &quot;OTHER&quot;))
))

# In a real setting, we would want to operate on full data.frames
# instead of only on atomic vectors.
</code></pre>

<p>It may seem silly to create a factor variable with a single value
and a surplus of unused levels, but that is only the case if you
have never tried to productionize your data science models! Remember,
even if you trained a simple regression, your factor columns will need
to be converted to 0/1 columns using something like the <code>model.matrix</code>
helper function, and this will yell at you if the correct levels are not
there on the factor column.</p>

<p>The point of mungebits is to replace all that hard work&ndash;which in the
experience of the author has sometimes spanned data preparation procedures
composed of <em>hundreds</em> of steps like the above for collections of
<em>thousands</em> of variables&ndash;with the much simplified</p>

<pre><code class="r"># During offline training.
replace_uncommon_levels_mungebit$run(dataset)
</code></pre>

<p>The mungebit has now been &ldquo;trained&rdquo; and remembers the <code>common_levels</code>
defined earlier. In a production system, we will be able to run the
exact same code on a single row of data, as long as we serialize
the mungebit object and recall it during production. This gives us
a streaming machine learning engine that includes hard data
wrangling work&ndash;in R.</p>

<pre><code class="r"># During real-time prediction.
replace_uncommon_levels_mungebit$run(dataset)
</code></pre>

<p>After understanding mungebits, data science will stop being data
janitor work and you will get back to the math.</p>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' Mungebits are atomic data transformations that are amenable to productionization.
#'
#' The majority of data projects are overcome by the burden of excessive
#' data wrangling. Part of the problem lies in the fact that when new
#' data is introduced that was drawn from the same source as the original,
#' such as a training set for a statistical model, \emph{different} code
#' needs to be written to achieve the same transformations. Mungebits solve
#' this problem by forcing the user to determine how to correctly munge
#' on out-of-sample data (such as live streaming data in the form of one-row
#' data.frames) at "munge-time", when the reason for the wrangling is still
#' apparent. A frequent source of data errors is treating this process as an
#' afterthought.
#'
#' Consider the following problem. Imagine we wish to discretize a variable,
#' say determined algorithmically with cuts [0, 0.5), [0.5, 1.5), [1.5, 3).
#' When we apply the same transformation on a new data set, we cannot run
#' the same discretization code, since it may produce new cutoffs, and hence
#' invalidate the results if, for example, we had trained a model on the
#' prior cutoffs. To ensure the exact same mathematical transformation
#' is performed on new data--whether a new test set derived from recent
#' data or a one-row data.frame representing a single record streaming
#' through a production system--we must run \emph{different code} on
#' the "original" set versus the new set.
#'
#' Mathematically speaking, a transformation of a data set can be represented
#' by a single mathematical function that is implemented differently during
#' "training" versus "prediction." Here, "training" refers to the first
#' time the transformation is performed, and "prediction" refers to 
#' subsequent times, such as on newly obtained data or a one-row data.frame
#' representing a single new record in a production system.
#'
#' Therefore, the \emph{correct} approach to data preparation, if you
#' wish to re-use it in the future on new data sets or in a live production
#' environment, is to treat it as a collection of tuples
#' \code{(train_function, predict_function, input)}, where
#' \code{train_function} represents the original code, \code{input} represents
#' an arbitrary R object such as a list, used for storing "metadata"
#' necessary to re-create the original transformation, and the
#' \code{predict_function} takes this \code{input} metadata and produces
#' the identical transformation on an out-of-sample data set.
#'
#' For example, if we wish to impute a data set, \code{train_function}
#' might compute the mean, store it in \code{input$mean}, replace
#' the \code{NA} values with the mean, and return the dataset. Meanwhile,
#' the \code{predict_function} simply replaces the \code{NA} values
#' with the cached \code{input$mean}.
#'
#' Usually, these steps would be in disjoint code bases: the modeler
#' would perform the ad-hoc munging while playing with the dataset,
#' and a software engineer would take the computed \code{input$mean}
#' and hard code it into a "data pipeline". It would be infeasible
#' to recompute the mean on-the-fly since \emph{it depends on the
#' original data set}, which may be prohibitively large. However,
#' while it may require a lot of space and time to compute the
#' original \code{input}, as they are parameterized potentially by
#' a very large data set, usually the \code{input} itself is small
#' and the resulting \code{predict_function} is inexpensive. 
#'
#' The fundamental problem of data preparation, and the reason why
#' \href{http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html}{data scientists spend over 90\% of their time on data preparation},
#' is a lack of respect for this dichotomy. Using mungebits makes
#' this duality blatantly apparent in all circumstances and will hopefully
#' reduce the amount of time wasted on cumbersome wrangling.
#'
#' @docType class
#' @format NULL
#' @name mungebit
#' @export
#' @examples
#' \dontrun{
#' mb <- mungebit(column_transformation(function(col, scale = NULL) {
#'   if (!isTRUE(trained)) { # trained is an injected keyword
#'    cat("Column scaled by ", input$scale, "\n")
#'   } else {
#'    input$scale <- scale
#'   }
#'  
#'   col * input$scale
#' }))
#' 
#' iris2 <- mb$run(iris, "Sepal.Length", 2)
#' # iris2 now contains a copy of iris with Sepal.Length doubled.
#' iris3 <- mb$run(iris2, "Sepal.Length")
#' # > Column scaled by 2
#' head(iris3[[1]] / iris[[1]])
#' # > [1] 4 4 4 4 4 4 
#' }
mungebit <- R6::R6Class("mungebit",
  public = list(
    .train_function   = NULL, # Function or NULL
    .predict_function = NULL, # Function or NULL
    .input            = NULL, # Environment
    .trained          = NULL, # Logical
    .enforce_train    = NULL, # Logical

    initialize = initialize,
    run        = run,
    train      = train,
    predict    = predict
  )
)
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>package.mungebits2.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">#' An approach to data preparation that is compatible with production systems.
#'
#' Mungebits2 defines a way of thinking about data preparation that
#' couples the definition of what happens in batch processing versus
#' online prediction so that both can be described by the same codebase.
#'
#' For example, consider the simple example of imputation. While the
#' general concept of imputing a variable works on arbitrary codebases,
#' a \emph{separate} data transformation will have to be defined for
#' each model that uses imputation in a production setting. This is 
#' because the imputed value depends inherently on the dataset.
#' We must remember the mean of the data set encountered during
#' training, and recall this value when performing replacement in
#' a production setting.
#'
#' Mungebits provide a sort of "train track switch" that allows one
#' to write data preparation offline, but ensure it works online 
#' (on a stream of new data, such as one-row data.frames).
#'
#' By reframing data preparation as the process of constructing
#' a "munge procedure", a list of trained mungebits that can
#' reproduce the same mathematical operation on a dataset in
#' a production environment without additional code, the process
#' of productionizing a machine learning model should become
#' significantly simplified.
#'
#' @name mungebits2
#' @import stagerunner crayon
#' @docType package
NULL</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>pending.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer">pending <- function() {
  TRUE
}
</span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            <h1>zzz.R</h1>

          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"></span></code>
            </pre>
          </div>
        </div>
        <div class="section">
          <div class="markdown markdown-body">
            
          </div>

          <div class="code">
            <pre>
              <code class="R"><span class="spacer"># TODO: (RK) Add notice if mungebits and mungebits2 are
# loaded at the same time.

.onLoad <- function(libPath, pkg) {
  if (as.package_version(R.version) < as.package_version("3.1.0")) {
    warning(crayon::red(paste0(
      "Using mungebits2 with R version < 3.1 will result ",
      "in dramatic performance slowdowns."
    )))
  }
}
</span></code>
            </pre>
          </div>
        </div>
      <div class="section">
      </div>

    </div>
  </body>
</html>
